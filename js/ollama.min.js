class OllamaAPI{constructor(server="http://localhost:11434"){this.server=server;this.defaultModelText="llama2:latest";this.defaultModelImage="llava:latest"}setServer(server){this.server=server}static async imgurl2base64(imgurl){return new Promise((resolve,reject)=>{const reader=new FileReader();reader.readAsDataURL(imgurl);reader.onload=()=>resolve(reader.result);reader.onerror=error=>reject(error)})}static escapeHTML(html){if(!html)return html;return html.replace(/</g,'&lt;').replace(/>/g,'&gt;')}async generate(prompt,callback=null,options={}){const{systemprompt=null,messages=[],images=[],model=null,responseOnly=!0,noHtml=!0,stream=!1}=options;if(!prompt){console.error("OLLAMA: no prompt provided");callback&&callback(!1);return}const selectedModel=model||(images.length>0?this.defaultModelImage:this.defaultModelText);const data={model:selectedModel,stream,options:options.options||{}};const chatMode=messages.length>0;const endpoint=chatMode?"/api/chat":"/api/generate";if(chatMode){data.messages=[...messages,{role:"user",content:prompt}];if(systemprompt){data.messages.unshift({role:"system",content:systemprompt})}}else{data.prompt=prompt;if(systemprompt){data.prompt=`${systemprompt}\n\n${data.prompt}`}}if(images.length>0){data.images=await Promise.all(images.map(async img=>img.startsWith("data:image")?img:await OllamaAPI.imgurl2base64(img)))}try{const response=await fetch(`${this.server}${endpoint}`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify(data)});const result=await response.json();if(result.error){console.error("OLLAMA DATA ERROR",result.error);callback&&callback(!1);return!1}let responseData=chatMode?(responseOnly?result.message.content:result):(responseOnly?result.response:result);if(noHtml&&responseOnly){responseData=OllamaAPI.escapeHTML(responseData)}callback&&callback(responseData);return responseData}catch(error){console.error("OLLAMA ERROR",error);callback&&callback(!1);return!1}}async getModels(callback){try{const response=await fetch(`${this.server}/api/tags`);const data=await response.json();callback&&callback(data.models);return data.models}catch(error){console.log("OLLAMA: NO MODELS LOADED",error);callback&&callback([]);return[]}}static getDefaultOptions(){return{"temperature":.8,"repeat_penalty":1.2}}}